{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "colab": {
   "name": "3 - Neural Machine Translation by Jointly Learning to Align and Translate.ipynb",
   "provenance": [],
   "include_colab_link": true
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/vasiliyeskin/bentrevett-pytorch-seq2seq_ru/blob/master/3%20-%20Neural%20Machine%20Translation%20by%20Jointly%20Learning%20to%20Align%20and%20Translate.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4g8xXDhqAOuX"
   },
   "source": [
    "# 3 - Neural Machine Translation by Jointly Learning to Align and Translate\n",
    "\n",
    "В этой третьей статье о моделях sequence-to-sequence с использованием PyTorch и torchText мы будем реализовывать модель из стать [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473). Эта модель демонстрирует лучшую точность из из трёх моделей (~27 по сравнению с ~34 у предыдущей модели).\n",
    "\n",
    "## Введение\n",
    "\n",
    "Напоминаем общую модель кодера-декодера:\n",
    "\n",
    "![](https://github.com/vasiliyeskin/bentrevett-pytorch-seq2seq_ru/blob/master/assets/seq2seq1.png?raw=1)\n",
    "\n",
    "В предыдущей модели наша архитектура была построена таким образом, чтобы уменьшить «сжатие информации» путем явной передачи вектора контекста $z$ в декодер на линейный слой $f$ на каждом временном шаге, совместно с передачей входного слова, прошедшего через слой эмбеддинга, $d(y_t)$ и со скрытым состоянием $s_t$.\n",
    "\n",
    "![](https://github.com/vasiliyeskin/bentrevett-pytorch-seq2seq_ru/blob/master/assets/seq2seq7.png?raw=1)\n",
    "\n",
    "Несмотря на то, что мы частично уменьшили сжатие информации, наш вектор контекста по-прежнему должен содержать всю информацию об исходном предложении. Модель, реализованная в этой статье, избегает такого сжатия, позволяя декодеру просматривать все исходное предложение (через его скрытые состояния) на каждом этапе декодирования! Как это стало возможным? Благодаря *вниманию*.\n",
    "\n",
    "Для использования механизма внимания, сначала вычисляем вектор внимания $a$. Каждый элемент вектора внимания находится в диапазоне от 0 до 1, а сумма элементов вектора равна 1. Затем мы вычисляем взвешенную сумму скрытых состояний исходного предложения $H$, чтобы получить взвешенный исходный вектор $w$.\n",
    "\n",
    "   $$w = \\sum_{i}a_ih_i$$\n",
    "\n",
    "Мы вычисляем новый взвешенный исходный вектор на каждом временном шаге при декодировании, используя его в качестве входных данных для RNN декодера, а также линейного слоя для прогнозирования. Мы объясним, как все это сделать далее.\n",
    "\n",
    "## Подготовка данных\n",
    "\n",
    "Снова подготовка аналогична прошлой.\n",
    "\n",
    "Сначала мы импортируем все необходимые модули."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "R4XB6wHLAOua"
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchtext.legacy.datasets import Multi30k\n",
    "from torchtext.legacy.data import Field, BucketIterator\n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xXDUR6g9AOub"
   },
   "source": [
    "Установите случайные значения для воспроизводимости."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "nRlDjalQAOuc"
   },
   "source": [
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n"
   ],
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Q-fEIe6AOud"
   },
   "source": [
    "Загрузите немецкую и английскую модели spaCy.\n",
    "\n",
    "```\n",
    "python -m spacy download en_core_web_sm\n",
    "python -m spacy download de_core_news_sm\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "JeLjDl3HSG0c"
   },
   "source": [
    "Для загрузки в Google Colab используем следующие команды (После загрузки обязательно перезапустите colab runtime! Наибыстрейший способ через короткую комаду： **Ctrl + M + .**):"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "HaGA5hCfSG0d"
   },
   "source": [
    "!pip install -U spacy==3.0\n",
    "!python -m spacy download en_core_web_sm\n",
    "!python -m spacy download de_core_news_sm\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "nFUPcEG5AOue"
   },
   "source": [
    "spacy_de = spacy.load('de_core_news_sm')\n",
    "spacy_en = spacy.load('en_core_web_sm')\n"
   ],
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5hCPYf_mAOuf"
   },
   "source": [
    "Создаем токенизаторы."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "tvnX-JAeAOug"
   },
   "source": [
    "def tokenize_de(text):\n",
    "    \"\"\"\n",
    "    Tokenizes German text from a string into a list of strings\n",
    "    \"\"\"\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    \"\"\"\n",
    "    Tokenizes English text from a string into a list of strings\n",
    "    \"\"\"\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]"
   ],
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tSNyknDEAOuh"
   },
   "source": [
    "Поля остаются теми же, что и раньше."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "vX-b65BFAOui"
   },
   "source": [
    "SRC = Field(tokenize = tokenize_de, \n",
    "            init_token = '<sos>', \n",
    "            eos_token = '<eos>', \n",
    "            lower = True)\n",
    "\n",
    "TRG = Field(tokenize = tokenize_en, \n",
    "            init_token = '<sos>', \n",
    "            eos_token = '<eos>', \n",
    "            lower = True)"
   ],
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dSGXjUbKAOuj"
   },
   "source": [
    "Загружаем данные."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "eO4vM4O4AOuk"
   },
   "source": [
    "train_data, valid_data, test_data = Multi30k.splits(exts = ('.de', '.en'), \n",
    "                                                    fields = (SRC, TRG))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "15WgJkNNAOul"
   },
   "source": [
    "Создаём словари."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "LN-E7V8gAOum"
   },
   "source": [
    "SRC.build_vocab(train_data, min_freq = 2)\n",
    "TRG.build_vocab(train_data, min_freq = 2)"
   ],
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hORJ9MHXAOun"
   },
   "source": [
    "Определяем устройство."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "93F-t2KpAOuo"
   },
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ],
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3-2G7hePAOuo"
   },
   "source": [
    "Создаём итераторы."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "PrkZDpq6AOuo"
   },
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size = BATCH_SIZE,\n",
    "    device = device)"
   ],
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uSO3t_ZSAOup"
   },
   "source": [
    "## Создание модели Seq2Seq\n",
    "\n",
    "### Кодер\n",
    "\n",
    "Сначала мы создадим кодер. Как и в предыдущей модели, мы используем только один слой GRU, однако теперь он будет иметь вид *двунаправленной RNN*. В двунаправленной RNN у нас есть две RNN на каждом уровне. *Вперёд-направленная RNN* перебирает предложение, прошедшее эмбеддинга, слева направо (показано ниже зеленым цветом), а *назад-направленная RNN* перебирает предложение, прошедшее эмбеддинг, справа налево (бирюзовый). Все, что нам нужно сделать в коде — установить `bidirectional = True`, а затем провести предложение через слой эмбеддинга в RNN, как и раньше.\n",
    "\n",
    "![](https://github.com/vasiliyeskin/bentrevett-pytorch-seq2seq_ru/blob/master/assets/seq2seq8.png?raw=1)\n",
    "\n",
    "Теперь у нас есть:\n",
    "\n",
    "$$\\begin{align*}\n",
    "h_t^\\rightarrow &= \\text{EncoderGRU}^\\rightarrow(e(x_t^\\rightarrow),h_{t-1}^\\rightarrow)\\\\\n",
    "h_t^\\leftarrow &= \\text{EncoderGRU}^\\leftarrow(e(x_t^\\leftarrow),h_{t-1}^\\leftarrow)\n",
    "\\end{align*}$$\n",
    "\n",
    "Где $x_0^\\rightarrow = \\text{<sos>}, x_1^\\rightarrow = \\text{guten}$ и $x_0^\\leftarrow = \\text{<eos>}, x_1^\\leftarrow = \\text{morgen}$.\n",
    "\n",
    "Как и раньше, мы передаем в RNN только ввод (`embedded`), который сообщает PyTorch о необходимости инициализировать как прямое, так и обратное начальные скрытые состояния ($h_0^\\rightarrow$ and $h_0^\\leftarrow$, respectively) тензором с нулевыми значениями элементов. Кроме того, мы получаем два вектора контекста: один из прямой RNN после того, как она увидит последнее слово в предложении $z^\\rightarrow=h_T^\\rightarrow$, а второй из обратной RNN после того, как она зафиксирует первое слово в предложении $z^\\leftarrow=h_T^\\leftarrow$.\n",
    "\n",
    "RNN возвращает `outputs` и `hidden`.\n",
    "\n",
    "`outputs` имеет размер **[src len, batch size, hid dim * num directions]** где первые `hid_dim` элементов в третьем измерении - это скрытые состояния от верхнего уровня вперёд-направленной RNN, а последнее `hid_dim` элементов — это скрытые состояния от верхнего уровня назад-направленной RNN. Мы можем думать о третьем измерении как о прямом и обратном скрытых состояниях, связанных вместе друг с другом, т.е. $h_1 = [h_1^\\rightarrow; h_{T}^\\leftarrow]$, $h_2 = [h_2^\\rightarrow; h_{T-1}^\\leftarrow]$ и мы можем обозначить все скрытые состояния кодировщика (прямое и обратное сцепление вместе) как $H=\\{ h_1, h_2, ..., h_T\\}$ (тензор контекста).\n",
    "\n",
    "`hidden` имеет размер **[n layers * num directions, batch size, hid dim]**, where **[-2, :, :]** дает скрытое состояние вперёд-направленной RNN верхнего уровня после последнего временного шага (т.е. после того, как он увидел последнее слово в предложении) и **[-1, :, :]** дает верхнему уровню скрытое состояние обратно-направленной RNN после последнего временного шага (т.е. после того, как он увидел первое слово в предложении).\n",
    "\n",
    "Поскольку декодер не является двунаправленным, ему нужен только один вектор контекста $z$ для использования в качестве начального скрытого состояния $s_0$, но в настоящее время у нас есть два вектора контекста ($z^\\rightarrow=h_T^\\rightarrow$ и $z^\\leftarrow=h_T^\\leftarrow$, respectively). Мы решаем эту проблему, объединив два вектора контекста вместе, пропустив их через линейный слой $g$ и применяя функцию активации $\\tanh$.\n",
    "\n",
    "$$\\begin{align*}\n",
    "z=\\tanh(g(z^\\rightarrow, z^\\leftarrow))\\\\\n",
    "z^\\rightarrow=h_T^\\rightarrow, z^\\leftarrow=h_T^\\leftarrow, z = s_0\n",
    "\\end{align*}$$\n",
    "\n",
    "**Замечание**: на самом деле здесь есть некоторое отклонение от реализации в статье. В статье авторы передают только первое (назад-направленной) скрытое состояние RNN через линейный слой, чтобы получить начальное скрытое состояние вектора контекста для декодера. Это кажется бессмысленным, поэтому мы изменили эту часть формирования вектора внимания.\n",
    "\n",
    "Поскольку мы хотим, чтобы наша модель просматривала все исходное предложение, мы возвращаем `outputs`, в виде объединённых скрытых состояний (вперед и назад) для каждого токена в исходном предложении. Мы возвращаем `hidden`, который действует как начальное скрытое состояние в декодере."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "-W59aQSZAOuq"
   },
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        \n",
    "        self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional = True)\n",
    "        \n",
    "        self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        \n",
    "        #src = [src len, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        \n",
    "        #embedded = [src len, batch size, emb dim]\n",
    "        \n",
    "        outputs, hidden = self.rnn(embedded)\n",
    "                \n",
    "        #outputs = [src len, batch size, hid dim * num directions]\n",
    "        #hidden = [n layers * num directions, batch size, hid dim]\n",
    "        \n",
    "        #hidden is stacked [forward_1, backward_1, forward_2, backward_2, ...]\n",
    "        #outputs are always from the last layer\n",
    "        \n",
    "        #hidden [-2, :, : ] is the last of the forwards RNN \n",
    "        #hidden [-1, :, : ] is the last of the backwards RNN\n",
    "        \n",
    "        #initial decoder hidden is final hidden state of the forwards and backwards \n",
    "        #  encoder RNNs fed through a linear layer\n",
    "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)))\n",
    "        \n",
    "        #outputs = [src len, batch size, enc hid dim * 2]\n",
    "        #hidden = [batch size, dec hid dim]\n",
    "        \n",
    "        return outputs, hidden"
   ],
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SRB9Aq9IAOur"
   },
   "source": [
    "### Внимание\n",
    "\n",
    "Далее идет слой внимания. Этот слой принимает предыдущее скрытое состояние декодера $s_{t-1}$ и все скрытые состояния кодера, собранные в тензор контекста $H$. Слой генерирует вектор внимания $a_t$ длины исходного предложения, каждый элемент которого находится в диапазоне от 0 до 1, а вся сумма элементов вектора равна 1.\n",
    "\n",
    "Интуитивно понятно, что этот слой берет то, что мы уже декодировали, $s_{t-1}$, и все, что мы закодировали в $H$, для создания вектора $a_t$, который представляет, каким словам в исходном предложении мы должны уделять большее внимание для правильного предсказать следующее слова декодировщиком, $\\hat{y}_{t+1}$.\n",
    "\n",
    "Сначала мы вычисляем *энергию взаимодействия* между предыдущим скрытым состоянием декодера и скрытыми состояниями кодера. Поскольку скрытые состояния нашего кодера представляют собой последовательность $T$ тензоров, и наше предыдущее скрытое состояние декодера — это одиночный тензор, первое, что мы делаем, это `повторяем` предыдущее скрытое состояние декодера $T$ раз. Затем мы вычисляем энергию взаимодействия $E_t$ между ними, объединив их вместе и пропустив через линейный слой (`attn`) и функцию активации $\\tanh$.\n",
    "\n",
    "$$E_t = \\tanh(\\text{attn}(s_{t-1}, H))$$ \n",
    "\n",
    "Эту величину можно рассматривать как вычисление того, насколько хорошо каждое скрытое состояние кодера «совпадает» с предыдущим скрытым состоянием декодера.\n",
    "\n",
    "В настоящее время у нас есть **[dec hid dim, src len]** тензор для каждого примера в батче. Мы хотим, чтобы он был длины **[src len]** для каждого примера в батче, так как внимание должно быть длины исходного предложения. Это достигается путем умножения `энергии` на **[1, dec hid dim]**-размерный тезор $v$.\n",
    "\n",
    "$$\\hat{a}_t = v E_t$$\n",
    "\n",
    "Мы можем думать о $v$ как о качестве весов взвешенной суммы энергии по всем скрытым состояниям кодировщика. Эти веса говорят нам, насколько мы должны уделять внимание каждому токену в исходной последовательности. Параметры $v$ инициализируются случайным образом, но изучаются вместе с остальной частью модели посредством обратного распространения ошибки. Обратите внимание, как $v$ не зависит от времени, и то же время $v$ используется для каждого временного шага декодирования. Реализуем $v$ как линейный слой без смещения.\n",
    "\n",
    "Наконец, мы следим за тем, чтобы вектор внимания соответствовал ограничениям, накладываемым на элементы этого вектора при передаче его через слой $\\text{softmax}$: все элементы находятся между 0 и 1, и суммирование элементов даёт 1.\n",
    "\n",
    "$$a_t = \\text{softmax}(\\hat{a_t})$$\n",
    "\n",
    "Это привлекает внимание к исходному предложению!\n",
    "\n",
    "Графически это выглядит примерно так, как показано ниже. Так для вычисления самого первого вектора внимания $s_{t-1} = s_0 = z$. Зеленые блоки представляют скрытые состояния как от вперёд-направленной, так и назад-направленной RNN, и все вычисления внимания выполняются в розовом блоке.\n",
    "\n",
    "![](https://github.com/vasiliyeskin/bentrevett-pytorch-seq2seq_ru/blob/master/assets/seq2seq9.png?raw=1)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "3b3617rHAOus"
   },
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, enc_hid_dim, dec_hid_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.attn = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, dec_hid_dim)\n",
    "        self.v = nn.Linear(dec_hid_dim, 1, bias = False)\n",
    "        \n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        \n",
    "        #hidden = [batch size, dec hid dim]\n",
    "        #encoder_outputs = [src len, batch size, enc hid dim * 2]\n",
    "        \n",
    "        batch_size = encoder_outputs.shape[1]\n",
    "        src_len = encoder_outputs.shape[0]\n",
    "        \n",
    "        #repeat decoder hidden state src_len times\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
    "        \n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        \n",
    "        #hidden = [batch size, src len, dec hid dim]\n",
    "        #encoder_outputs = [batch size, src len, enc hid dim * 2]\n",
    "        \n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim = 2))) \n",
    "        \n",
    "        #energy = [batch size, src len, dec hid dim]\n",
    "\n",
    "        attention = self.v(energy).squeeze(2)\n",
    "        \n",
    "        #attention= [batch size, src len]\n",
    "        \n",
    "        return F.softmax(attention, dim=1)"
   ],
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aP-SwAHJAOus"
   },
   "source": [
    "### Декодер\n",
    "\n",
    "Далее идет декодер.\n",
    "\n",
    "Декодер содержит слой внимания, `attention`, который принимает предыдущее скрытое состояние $s_{t-1}$, все скрытые состояния кодировщика $H$, и возвращает вектор внимания $a_t$.\n",
    "\n",
    "Затем мы используем этот вектор внимания для создания взвешенного исходного вектора $w_t$, который обозначается как `weighted`, который представляет собой взвешенную сумму скрытых состояний кодировщика $H$, использованный совместно с весами $a_t$.\n",
    "\n",
    "$$w_t = a_t H$$\n",
    "\n",
    "Входное слово, прошедшее эмбеддинга $d(y_t)$, взвешенный исходный вектор $w_t$, и предыдущее скрытое состояние декодера $s_{t-1}$, все это передаются в декодер RNN, с $d(y_t)$ и $w_t$ и соединяется вместе.\n",
    "\n",
    "$$s_t = \\text{DecoderGRU}(d(y_t), w_t, s_{t-1})$$\n",
    "\n",
    "Затем мы передаем $d(y_t)$, $w_t$ и $s_t$ через линейный слой $f$, для совершения предсказания следующего слова в целевом предложении $\\hat{y}_{t+1}$. Это делается путем их объединения.\n",
    "\n",
    "$$\\hat{y}_{t+1} = f(d(y_t), w_t, s_t)$$\n",
    "\n",
    "На изображении ниже показано декодирование первого слова в примере перевода.\n",
    "\n",
    "![](https://github.com/vasiliyeskin/bentrevett-pytorch-seq2seq_ru/blob/master/assets/seq2seq10.png?raw=1)\n",
    "\n",
    "Зелёные/бирюзовый блоки показывают RNNs кодера которые выдают $H$, красный блок показывает вектор контекста, $z = h_T = \\tanh(g(h^\\rightarrow_T,h^\\leftarrow_T)) = \\tanh(g(z^\\rightarrow, z^\\leftarrow)) = s_0$, синий блок показывает RNN декодера, который выводит $s_t$, фиолетовый блок показывает линейный слой $f$,который выводит $\\hat{y}_{t+1}$ а оранжевый блок показывает вычисление взвешенной суммы по $H$ от $a_t$ и выходов $w_t$. Не показан расчет $a_t$."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "yfTsFTZ7AOut"
   },
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, attention):\n",
    "        super().__init__()\n",
    "\n",
    "        self.output_dim = output_dim\n",
    "        self.attention = attention\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        \n",
    "        self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim)\n",
    "        \n",
    "        self.fc_out = nn.Linear((enc_hid_dim * 2) + dec_hid_dim + emb_dim, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "             \n",
    "        #input = [batch size]\n",
    "        #hidden = [batch size, dec hid dim]\n",
    "        #encoder_outputs = [src len, batch size, enc hid dim * 2]\n",
    "        \n",
    "        input = input.unsqueeze(0)\n",
    "        \n",
    "        #input = [1, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        \n",
    "        #embedded = [1, batch size, emb dim]\n",
    "        \n",
    "        a = self.attention(hidden, encoder_outputs)\n",
    "                \n",
    "        #a = [batch size, src len]\n",
    "        \n",
    "        a = a.unsqueeze(1)\n",
    "        \n",
    "        #a = [batch size, 1, src len]\n",
    "        \n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        \n",
    "        #encoder_outputs = [batch size, src len, enc hid dim * 2]\n",
    "        \n",
    "        weighted = torch.bmm(a, encoder_outputs)\n",
    "        \n",
    "        #weighted = [batch size, 1, enc hid dim * 2]\n",
    "        \n",
    "        weighted = weighted.permute(1, 0, 2)\n",
    "        \n",
    "        #weighted = [1, batch size, enc hid dim * 2]\n",
    "        \n",
    "        rnn_input = torch.cat((embedded, weighted), dim = 2)\n",
    "        \n",
    "        #rnn_input = [1, batch size, (enc hid dim * 2) + emb dim]\n",
    "            \n",
    "        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\n",
    "        \n",
    "        #output = [seq len, batch size, dec hid dim * n directions]\n",
    "        #hidden = [n layers * n directions, batch size, dec hid dim]\n",
    "        \n",
    "        #seq len, n layers and n directions will always be 1 in this decoder, therefore:\n",
    "        #output = [1, batch size, dec hid dim]\n",
    "        #hidden = [1, batch size, dec hid dim]\n",
    "        #this also means that output == hidden\n",
    "        assert (output == hidden).all()\n",
    "        \n",
    "        embedded = embedded.squeeze(0)\n",
    "        output = output.squeeze(0)\n",
    "        weighted = weighted.squeeze(0)\n",
    "        \n",
    "        prediction = self.fc_out(torch.cat((output, weighted, embedded), dim = 1))\n",
    "        \n",
    "        #prediction = [batch size, output dim]\n",
    "        \n",
    "        return prediction, hidden.squeeze(0)"
   ],
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q0EaN2PhAOuu"
   },
   "source": [
    "### Seq2Seq\n",
    "\n",
    "Это первая модель, в которой нам не нужно, чтобы RNN кодировщика и RNN декодера имели одинаковые скрытые размеры, однако кодировщик должен быть двунаправленным. Последнее требование можно игнорировать, изменив все размерность входных данных с `enc_dim * 2` на `enc_dim * 2 if encoder_is_bidirectional else enc_dim`.\n",
    "\n",
    "Эта модель seq2seq инкапсулирует кодер и декодер как и в двух предыдущих моделях. Единственная разница в том, что `encoder` возвращает как окончательное скрытое состояние (который является окончательным скрытым состоянием как от вперёд-направленного, так и от назад-направленного RNN кодировщика, прошедших через линейный уровень) для использования в качестве начального скрытого состояния в декодере, а также для каждого скрытого состояния (которые представляют собой скрытые состояния на выходе вперёд- и назад-направленные RNNN, накладываемые друг на друга). Нам также необходимо обеспечить, чтобы `hidden` и `encoder_outputs` передавались в декодер.\n",
    "\n",
    "Кратко пройдемся по всем этапам:\n",
    "- тензор `outputs` создан для хранения всех прогнозов $\\hat{Y}$\n",
    "- исходная последовательность $X$, подается в кодировщик для получения $z$ и $H$\n",
    "- начальное скрытое состояние декодера установлено как вектор `context` $s_0 = z = h_T$\n",
    "- мы используем батч токенов `<sos>` как первый `input` $y_1$\n",
    "- затем декодируем в цикле:\n",
    "  - вставка входного токена $y_t$, предыдущее скрытое состояние $s_{t-1}$, и все выходы кодера $H$ в декодер\n",
    "  - получение прогноза $\\hat{y}_{t+1}$ и новое скрытое состояние $s_t$\n",
    "  - затем мы решаем, собираемся ли мы применять обучение с принуждением или нет, устанавливая следующий ввод соответствующим образом"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "dvyljaJxAOuv"
   },
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
    "        \n",
    "        #src = [src len, batch size]\n",
    "        #trg = [trg len, batch size]\n",
    "        #teacher_forcing_ratio is probability to use teacher forcing\n",
    "        #e.g. if teacher_forcing_ratio is 0.75 we use teacher forcing 75% of the time\n",
    "        \n",
    "        batch_size = src.shape[1]\n",
    "        trg_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        \n",
    "        #tensor to store decoder outputs\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        \n",
    "        #encoder_outputs is all hidden states of the input sequence, back and forwards\n",
    "        #hidden is the final forward and backward hidden states, passed through a linear layer\n",
    "        encoder_outputs, hidden = self.encoder(src)\n",
    "                \n",
    "        #first input to the decoder is the <sos> tokens\n",
    "        input = trg[0,:]\n",
    "        \n",
    "        for t in range(1, trg_len):\n",
    "            \n",
    "            #insert input token embedding, previous hidden state and all encoder hidden states\n",
    "            #receive output tensor (predictions) and new hidden state\n",
    "            output, hidden = self.decoder(input, hidden, encoder_outputs)\n",
    "            \n",
    "            #place predictions in a tensor holding predictions for each token\n",
    "            outputs[t] = output\n",
    "            \n",
    "            #decide if we are going to use teacher forcing or not\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            \n",
    "            #get the highest predicted token from our predictions\n",
    "            top1 = output.argmax(1) \n",
    "            \n",
    "            #if teacher forcing, use actual next token as next input\n",
    "            #if not, use predicted token\n",
    "            input = trg[t] if teacher_force else top1\n",
    "\n",
    "        return outputs"
   ],
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sv9nfcKGAOuw"
   },
   "source": [
    "## Обучение модели Seq2Seq\n",
    "\n",
    "Остальная часть этого урока очень похожа на предыдущий.\n",
    "\n",
    "Мы инициализируем наши параметры, кодера, декодер и модели seq2seq  (поместив его на графический процессор, если он у нас есть)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "UQkKPoq-AOuw"
   },
   "source": [
    "INPUT_DIM = len(SRC.vocab)\n",
    "OUTPUT_DIM = len(TRG.vocab)\n",
    "ENC_EMB_DIM = 256\n",
    "DEC_EMB_DIM = 256\n",
    "ENC_HID_DIM = 512\n",
    "DEC_HID_DIM = 512\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "\n",
    "attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n",
    "\n",
    "model = Seq2Seq(enc, dec, device).to(device)"
   ],
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s-jxo-mWAOux"
   },
   "source": [
    "Мы используем упрощенную версию схемы инициализации весов, использованную в статье. Здесь мы инициализируем все смещения равными нулю и все веса из $\\mathcal{N}(0, 0.01)$."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "o7IE1pknAOux"
   },
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "        else:\n",
    "            nn.init.constant_(param.data, 0)\n",
    "            \n",
    "model.apply(init_weights)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HVElHrR2AOux"
   },
   "source": [
    "Подсчитаем количество параметров. Получаем прибавку почти 50% по сравнению с количеством параметров из последней модели."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "DzpBPc6hAOuy",
    "outputId": "1f4f7822-9604-4157-8ef4-cc1f8711c2df",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ],
   "execution_count": 16,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "The model has 20,518,405 trainable parameters\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iHdE7eP7AOuz"
   },
   "source": [
    "Создаем оптимизатор."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "j9APVV2hAOuz"
   },
   "source": [
    "optimizer = optim.Adam(model.parameters())"
   ],
   "execution_count": 17,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jQuBt5EHAOuz"
   },
   "source": [
    "Инициализируем функцию потерь."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ISE1MSO8AOu0"
   },
   "source": [
    "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"
   ],
   "execution_count": 18,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uFkHCs5aAOu0"
   },
   "source": [
    "Затем мы создаем цикл обучения ..."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "D4pU7Z_JAOu0"
   },
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, batch in enumerate(iterator):\n",
    "        \n",
    "        src = batch.src\n",
    "        trg = batch.trg\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(src, trg)\n",
    "        \n",
    "        #trg = [trg len, batch size]\n",
    "        #output = [trg len, batch size, output dim]\n",
    "        \n",
    "        output_dim = output.shape[-1]\n",
    "        \n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        trg = trg[1:].view(-1)\n",
    "        \n",
    "        #trg = [(trg len - 1) * batch size]\n",
    "        #output = [(trg len - 1) * batch size, output dim]\n",
    "        \n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ],
   "execution_count": 19,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6aWTg9s9AOu1"
   },
   "source": [
    "...и цикл оценки, не забывая установить модель на `eval` режим и отключив обучение с принуждением."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "usLS3g3bAOu1"
   },
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i, batch in enumerate(iterator):\n",
    "\n",
    "            src = batch.src\n",
    "            trg = batch.trg\n",
    "\n",
    "            output = model(src, trg, 0) #turn off teacher forcing\n",
    "\n",
    "            #trg = [trg len, batch size]\n",
    "            #output = [trg len, batch size, output dim]\n",
    "\n",
    "            output_dim = output.shape[-1]\n",
    "            \n",
    "            output = output[1:].view(-1, output_dim)\n",
    "            trg = trg[1:].view(-1)\n",
    "\n",
    "            #trg = [(trg len - 1) * batch size]\n",
    "            #output = [(trg len - 1) * batch size, output dim]\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ],
   "execution_count": 20,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S-JnFOfdAOu2"
   },
   "source": [
    "Наконец, определим функцию подсчёта времени."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Rh6N3gSrAOu2"
   },
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ],
   "execution_count": 21,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4k-bd53nAOu3"
   },
   "source": [
    "Затем мы обучаем нашу модель, сохраняя параметры, которые дают нам наименьшие потери при проверке."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "YW7CeRq3AOu3",
    "outputId": "41d74946-c7d4-432c-a518-354739ac5817",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "source": [
    "N_EPOCHS = 10\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut3-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
   ],
   "execution_count": 22,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 1m 17s\n",
      "\tTrain Loss: 5.020 | Train PPL: 151.388\n",
      "\t Val. Loss: 4.813 |  Val. PPL: 123.099\n",
      "Epoch: 02 | Time: 1m 20s\n",
      "\tTrain Loss: 4.127 | Train PPL:  61.981\n",
      "\t Val. Loss: 4.602 |  Val. PPL:  99.675\n",
      "Epoch: 03 | Time: 1m 22s\n",
      "\tTrain Loss: 3.470 | Train PPL:  32.151\n",
      "\t Val. Loss: 3.779 |  Val. PPL:  43.776\n",
      "Epoch: 04 | Time: 1m 24s\n",
      "\tTrain Loss: 2.914 | Train PPL:  18.436\n",
      "\t Val. Loss: 3.430 |  Val. PPL:  30.879\n",
      "Epoch: 05 | Time: 1m 24s\n",
      "\tTrain Loss: 2.524 | Train PPL:  12.480\n",
      "\t Val. Loss: 3.329 |  Val. PPL:  27.917\n",
      "Epoch: 06 | Time: 1m 25s\n",
      "\tTrain Loss: 2.235 | Train PPL:   9.346\n",
      "\t Val. Loss: 3.258 |  Val. PPL:  25.996\n",
      "Epoch: 07 | Time: 1m 24s\n",
      "\tTrain Loss: 1.990 | Train PPL:   7.314\n",
      "\t Val. Loss: 3.145 |  Val. PPL:  23.219\n",
      "Epoch: 08 | Time: 1m 24s\n",
      "\tTrain Loss: 1.774 | Train PPL:   5.895\n",
      "\t Val. Loss: 3.224 |  Val. PPL:  25.138\n",
      "Epoch: 09 | Time: 1m 24s\n",
      "\tTrain Loss: 1.606 | Train PPL:   4.982\n",
      "\t Val. Loss: 3.269 |  Val. PPL:  26.288\n",
      "Epoch: 10 | Time: 1m 24s\n",
      "\tTrain Loss: 1.493 | Train PPL:   4.453\n",
      "\t Val. Loss: 3.338 |  Val. PPL:  28.166\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z0kV4XrRAOu4"
   },
   "source": [
    "Наконец, мы тестируем модель на тестовой выборке, используя эти «лучшие» параметры."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Em8Z5CC1AOu4",
    "outputId": "0c94ef58-358e-4bc8-c272-3f6ec3759260",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "source": [
    "model.load_state_dict(torch.load('tut3-model.pt'))\n",
    "\n",
    "test_loss = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
   ],
   "execution_count": 23,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "| Test Loss: 3.159 | Test PPL:  23.545 |\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ffAwNDtsAOu5"
   },
   "source": [
    "Мы улучшили предыдущую модель, но это произошло за счет удвоения времени обучения.\n",
    "\n",
    "В следующей статье мы будем использовать ту же архитектуру, но применим несколько приемов ко всем архитектурам RNN - упакованные дополненные последовательности и маскирование. Мы реализуем код, который позволит нам посмотреть, на какие слова во входных данных RNN обращает внимание при декодировании выходных данных."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "fVb9xXs_XB3_",
    "outputId": "12293e6e-9521-485f-b77c-c95711f29973",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "source": [
    "!nvidia-smi"
   ],
   "execution_count": 25,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Mon Jun  7 21:23:57 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 465.27       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   49C    P0    27W /  70W |   5460MiB / 15109MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "cGho9HbQXDln"
   },
   "source": [
    ""
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}