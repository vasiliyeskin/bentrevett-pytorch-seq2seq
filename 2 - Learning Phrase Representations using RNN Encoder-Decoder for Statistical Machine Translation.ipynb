{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "colab": {
   "name": "2 - Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation.ipynb",
   "provenance": [],
   "include_colab_link": true
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/vasiliyeskin/bentrevett-pytorch-seq2seq_ru/blob/master/2%20-%20Learning%20Phrase%20Representations%20using%20RNN%20Encoder-Decoder%20for%20Statistical%20Machine%20Translation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fUH1rgB8PjPp"
   },
   "source": [
    "# 2 - Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation\n",
    "\n",
    "Во второй статье о моделях sequence-to-sequence с использованием PyTorch и TorchText мы будем реализовывать модель из работы [Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation](https://arxiv.org/abs/1406.1078). Эта модель позволит достичь улучшенной тестовой сложности (perplexity) при использовании только однослойной RNN как в кодере, так и в декодере.\n",
    "\n",
    "## Введение\n",
    "\n",
    "Напомним общую модель кодера-декодера.\n",
    "\n",
    "![](https://github.com/vasiliyeskin/bentrevett-pytorch-seq2seq_ru/blob/master/assets/seq2seq1.png?raw=1)\n",
    "\n",
    "Мы используем наш кодировщик (зеленый) поверх исходной последовательности, прошедшей слой эмбеддинга (желтый), чтобы создать вектор контекста (красный). Затем мы передаём этот вектор контекста в декодер (синий) с линейным слоем (фиолетовый) для генерации целевого предложения.\n",
    "\n",
    "В предыдущей модели мы использовали многослойную LSTM сеть в качестве кодера и декодера.\n",
    "\n",
    "![](https://github.com/vasiliyeskin/bentrevett-pytorch-seq2seq_ru/blob/master/assets/seq2seq4.png?raw=1)\n",
    "\n",
    "Одним из недостатков предыдущей модели является то, что декодер пытается втиснуть большое количество информации в скрытые состояния. Во время декодирования скрытое состояние должно содержать информацию обо всей исходной последовательности, а также обо всех токенах, которые были декодированы на данный момент. Улучшив сжатие этой информации, мы сможем создать улучшенную модель!\n",
    "\n",
    "Мы будем использовать сеть GRU (Gated Recurrent Unit) вместо LSTM (Long Short-Term Memory). Почему? В основном потому, что так авторы сделали в статье (в этой же статье была представлена GRU сеть), а также потому, что в прошлый раз мы использовали LSTM. Отличия GRU (и LSTM) от стандартных RNN подробно рассмотрены [здесь](https://colah.github.io/posts/2015-08-Understanding-LSTMs/). Резонный вопрос, GRU лучше LSTM? [Исследование](https://arxiv.org/abs/1412.3555) показало, что они почти одинаковы и одновременно, чем стандартные RNN.\n",
    "\n",
    "## Подготовка данных\n",
    "\n",
    "Вся подготовка данных будет (почти) такой же, как и в прошлый раз, поэтому мы очень кратко опишем, что делает каждый блок кода. Более развётнутое описание смотрите в предыдущей статье.\n",
    "\n",
    "Мы импортируем PyTorch, TorchText, spaCy и несколько стандартных модулей."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "T5AvrjN-PjPx"
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchtext.legacy.datasets import Multi30k\n",
    "from torchtext.legacy.data import Field, BucketIterator\n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zZwh4mM3PjP1"
   },
   "source": [
    "Затем установите случайное начальное число для детерминированной воспроизводимости результатов."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "LpykfqpXPjP2"
   },
   "source": [
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "tiLJiduRPjP4"
   },
   "source": [
    "Для Google Colab используем следующие команды (После загрузки не забывайте перезапустите colab runtime! Наибыстрейший способ через короткую комаду： **Ctrl + M + .**):"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "xGGvzJUQPjP5"
   },
   "source": [
    "!pip install -U spacy==3.0\n",
    "!python -m spacy download en_core_web_sm\n",
    "!python -m spacy download de_core_news_sm"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1osFhmvaPjP2"
   },
   "source": [
    "Создаём экземпляры наших немецких и английских spaCy моделей."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Ikq-_tE8PjP3"
   },
   "source": [
    "spacy_de = spacy.load('de_core_news_sm')\n",
    "spacy_en = spacy.load('en_core_web_sm')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ph4hl642PjP6"
   },
   "source": [
    "Ранее мы поменяли местами исходное (немецкое) предложение, однако в статье, которую мы реализуем, они этого не делают, и мы не будем."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "i-eOxqr4PjP7"
   },
   "source": [
    "def tokenize_de(text):\n",
    "    \"\"\"\n",
    "    Tokenizes German text from a string into a list of strings\n",
    "    \"\"\"\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    \"\"\"\n",
    "    Tokenizes English text from a string into a list of strings\n",
    "    \"\"\"\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2PpKpZWzPjP8"
   },
   "source": [
    "Далее мы создаем токенизирующие функции. Они могут быть переданы в torchtext и будут принимать предложение в виде строки, а возвращать предложение в виде списка токенов."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "wTGflumEPjP9"
   },
   "source": [
    "SRC = Field(tokenize=tokenize_de, \n",
    "            init_token='<sos>', \n",
    "            eos_token='<eos>', \n",
    "            lower=True)\n",
    "\n",
    "TRG = Field(tokenize = tokenize_en, \n",
    "            init_token='<sos>', \n",
    "            eos_token='<eos>', \n",
    "            lower=True)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oyM0ovjTPjQA"
   },
   "source": [
    "Загрузка наших данных."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "35FQFEtBPjQA"
   },
   "source": [
    "train_data, valid_data, test_data = Multi30k.splits(exts = ('.de', '.en'), \n",
    "                                                    fields = (SRC, TRG))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GPC_4TkEPjQB"
   },
   "source": [
    "Мы распечатаем пример, чтобы проверить, не перевернут ли он."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "lLFCon-BPjQB",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "6393e587-6efb-453b-ee05-c8029161d673"
   },
   "source": [
    "print(vars(train_data.examples[0]))"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "{'src': ['zwei', 'junge', 'weiße', 'männer', 'sind', 'im', 'freien', 'in', 'der', 'nähe', 'vieler', 'büsche', '.'], 'trg': ['two', 'young', ',', 'white', 'males', 'are', 'outside', 'near', 'many', 'bushes', '.']}\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zKMbzpMlPjQC"
   },
   "source": [
    "Затем создаём наш словарь, преобразовав все токены, встречающиеся менее двух раз, в `<unk>` токены."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "VmEULPAGPjQC"
   },
   "source": [
    "SRC.build_vocab(train_data, min_freq = 2)\n",
    "TRG.build_vocab(train_data, min_freq = 2)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WTlJqQRgPjQD"
   },
   "source": [
    "Наконец, определим `device` и создаём наши итераторы."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "-n_Xhrq1PjQE"
   },
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ABPFpDPIPjQE"
   },
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size = BATCH_SIZE, \n",
    "    device = device)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2dozd9zLPjQF"
   },
   "source": [
    "## Создание Seq2Seq модели\n",
    "\n",
    "### Кодер\n",
    "\n",
    "Кодер аналогичен предыдущему, но многослойный LSTM заменен на однослойный GRU. Кроме того, мы не передаем дропаут в качестве аргумента GRU, поскольку этот дропаут используется между слоями многослойной RNN. Поскольку у нас есть только один слой, PyTorch отобразит предупреждение, если мы попытаемся передать ему значение дропаута.\n",
    "\n",
    "Еще одна вещь, которую следует отметить в отношении GRU, заключается в том, что он требует и возвращает только скрытое состояние, не нуждаясь в состоянии ячейки, как в LSTM.\n",
    "\n",
    "$$\\begin{align*}\n",
    "h_t &= \\text{GRU}(e(x_t), h_{t-1})\\\\\n",
    "(h_t, c_t) &= \\text{LSTM}(e(x_t), h_{t-1}, c_{t-1})\\\\\n",
    "h_t &= \\text{RNN}(e(x_t), h_{t-1})\n",
    "\\end{align*}$$\n",
    "\n",
    "Из приведенных выше уравнений видно, что RNN и GRU идентичны. Однако внутри GRU есть несколько *запорных механизмовs*, которые контролируют поток информации в скрытое состояние и из него (похожий на LSTM). Опять же, для получения дополнительной информации обращайтесь [сюда](https://colah.github.io/posts/2015-08-Understanding-LSTMs/).\n",
    "\n",
    "Остальная часть кодировщика должна быть очень похожа на случай предыдущей статьи: кодер принимает последовательность $X = \\{x_1, x_2, ... , x_T\\}$, передает её через слой эмбеддинга, периодически вычисляя скрытые состояния $H = \\{h_1, h_2, ..., h_T\\}$, и возвращает вектор контекста (окончательное скрытое состояние) $z=h_T$.\n",
    "\n",
    "$$h_t = \\text{EncoderGRU}(e(x_t), h_{t-1})$$\n",
    "\n",
    "Он идентичен кодировщику общей модели seq2seq, со всей \"магией\", происходящей внутри GRU (зеленый).\n",
    "\n",
    "![](https://github.com/vasiliyeskin/bentrevett-pytorch-seq2seq_ru/blob/master/assets/seq2seq5.png?raw=1)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "P66XVBA2PjQG"
   },
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hid_dim = hid_dim\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim) #no dropout as only one layer!\n",
    "        \n",
    "        self.rnn = nn.GRU(emb_dim, hid_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        \n",
    "        #src = [src len, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        \n",
    "        #embedded = [src len, batch size, emb dim]\n",
    "        \n",
    "        outputs, hidden = self.rnn(embedded) #no cell state!\n",
    "        \n",
    "        #outputs = [src len, batch size, hid dim * n directions]\n",
    "        #hidden = [n layers * n directions, batch size, hid dim]\n",
    "        \n",
    "        #outputs are always from the top hidden layer\n",
    "        \n",
    "        return hidden"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nE8w2URwPjQH"
   },
   "source": [
    "## Декодер\n",
    "Реализация данного декодер значительно отличается от декодера предыдущей модели, и в текущем декодере мы усилили сжатие некоторой информации.\n",
    "\n",
    "GRU в декодере принимает не только целевой токен из эмбеддинга $d(y_t)$ и предыдущее скрытое состояние $s_{t-1}$ в качестве входных данных, но также и вектор контекста $z$.\n",
    "\n",
    "$$s_t = \\text{DecoderGRU}(d(y_t), s_{t-1}, z)$$\n",
    "\n",
    "Обратите внимание, как этот вектор контекста $z$ не имеет индекса $t$, это означает, что мы повторно используем один и тот же вектор контекста, возвращаемый кодировщиком, для каждого временного шага в декодере.\n",
    "\n",
    "Раньше мы предсказывали следующий токен $\\hat{y}_{t+1}$ с линейным слоем $f$, используя только выход декодер верхнего уровня, скрытый на этом временном шаге $s_t$ как $\\hat{y}_{t+1}=f(s_t^L)$. Теперь мы также передаем текущий токен эмбеддинга $d(y_t)$ и вектор контекста $z$ в линейный слой.\n",
    "\n",
    "$$\\hat{y}_{t+1} = f(d(y_t), s_t, z)$$\n",
    "\n",
    "Таким образом, наш декодер теперь выглядит примерно так:\n",
    "\n",
    "![](https://github.com/vasiliyeskin/bentrevett-pytorch-seq2seq_ru/blob/master/assets/seq2seq6.png?raw=1)\n",
    "\n",
    "Обратите внимание, начальное скрытое состояние $s_0$ по-прежнему является вектором контекста $z$, поэтому при генерации первого токена мы фактически вводим два идентичных вектора контекста в GRU.\n",
    "\n",
    "Как эти два изменения уменьшают сжатие информации? Гипотетически скрытым состояниям декодер $s_t$ больше нет необходимости содержать информацию об исходной последовательности, поскольку она всегда доступна в качестве входных данных. Таким образом, они должены содержать только информацию о том, какие токены они уже сгенерировали. Передача $y_t$ в линейный уровень (через эмбеддинг $d(y_t)$) означает, что этот уровень может напрямую видеть входной токен, без необходимости получать информацию о нём из скрытого состояния.\n",
    "\n",
    "Однако эта гипотеза — всего лишь гипотеза, невозможно определить, как модель на самом деле использует предоставленную ей информацию (не слушайте никого, кто говорит иначе). Тем не менее это хорошая догадка, и результаты, кажется, указывают на то, что эта модификации является хорошей идеей!\n",
    "\n",
    "В рамках реализации мы передадим $d(y_t)$ и $z$ в GRU объединив их вместе, так что входные размеры в GRU были `emb_dim + hid_dim` (поскольку вектор контекста будет иметь размер `hid_dim`). Линейный слой принимает $d(y_t)$, $s_t$ и $z$ объединения их вместе, поэтому входные размеры теперь `emb_dim + hid_dim*2`. Мы также не передаем значение дропаута в GRU, поскольку оно используется только на входном уровене.\n",
    "\n",
    "`forward` сейчас принимает аргумента `context`. Внутри `forward` мы объединяем $y_t$ и $z$ как `emb_con` перед \"скармливанием\" GRU, и объединяем $d(y_t)$, $s_t$ и $z$ вместе как `output` перед тем, как пропустить его через линейный слой, чтобы получить наши прогнозы $\\hat{y}_{t+1}$."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "nNjgnDfpPjQI"
   },
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hid_dim = hid_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        \n",
    "        self.rnn = nn.GRU(emb_dim + hid_dim, hid_dim)\n",
    "        \n",
    "        self.fc_out = nn.Linear(emb_dim + hid_dim * 2, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input, hidden, context):\n",
    "        \n",
    "        #input = [batch size]\n",
    "        #hidden = [n layers * n directions, batch size, hid dim]\n",
    "        #context = [n layers * n directions, batch size, hid dim]\n",
    "        \n",
    "        #n layers and n directions in the decoder will both always be 1, therefore:\n",
    "        #hidden = [1, batch size, hid dim]\n",
    "        #context = [1, batch size, hid dim]\n",
    "        \n",
    "        input = input.unsqueeze(0)\n",
    "        \n",
    "        #input = [1, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        \n",
    "        #embedded = [1, batch size, emb dim]\n",
    "                \n",
    "        emb_con = torch.cat((embedded, context), dim = 2)\n",
    "            \n",
    "        #emb_con = [1, batch size, emb dim + hid dim]\n",
    "            \n",
    "        output, hidden = self.rnn(emb_con, hidden)\n",
    "        \n",
    "        #output = [seq len, batch size, hid dim * n directions]\n",
    "        #hidden = [n layers * n directions, batch size, hid dim]\n",
    "        \n",
    "        #seq len, n layers and n directions will always be 1 in the decoder, therefore:\n",
    "        #output = [1, batch size, hid dim]\n",
    "        #hidden = [1, batch size, hid dim]\n",
    "        \n",
    "        output = torch.cat((embedded.squeeze(0), hidden.squeeze(0), context.squeeze(0)), \n",
    "                           dim = 1)\n",
    "        \n",
    "        #output = [batch size, emb dim + hid dim * 2]\n",
    "        \n",
    "        prediction = self.fc_out(output)\n",
    "        \n",
    "        #prediction = [batch size, output dim]\n",
    "        \n",
    "        return prediction, hidden"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jpv05x8FPjQJ"
   },
   "source": [
    "## Seq2Seq модель\n",
    "\n",
    "Соединяя кодировщик и декодер, получаем:\n",
    "\n",
    "![](https://github.com/vasiliyeskin/bentrevett-pytorch-seq2seq_ru/blob/master/assets/seq2seq7.png?raw=1)\n",
    "\n",
    "Снова, в этой реализации нам нужно обеспечить одинаковые скрытые размеры в кодировщике и декодере.\n",
    "\n",
    "Кратко пройдемся по всем этапам:\n",
    "- тензор `outputs` создан для хранения всех прогнозов $\\hat{Y}$\n",
    "- исходная последовательность $X$ подается в кодировщик для получения вектора контекста `context`\n",
    "- начальное скрытое состояние декодера установлено как вектор `context` $s_0 = z = h_T$\n",
    "- мы используем `<sos>` в качестве входных токенов `input` $y_1$\n",
    "- затем декодируем в цикле:\n",
    "  - передача входного токена $y_t$, предыдущего скрытого состояния $s_{t-1}$, и вектора контекста $z$ в декодер\n",
    "  - получение прогноза $\\hat{y}_{t+1}$ и нового скрытого состояния $s_t$\n",
    "  - Затем мы решаем, собираемся ли мы использовать обучение с принуждением или нет, устанавливая следующий вход соответствующим образом (либо следующий истинный токен в целевой последовательности, либо самый вероятный следующий токен)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "o4grjh9_PjQJ"
   },
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "        assert encoder.hid_dim == decoder.hid_dim, \\\n",
    "            \"Hidden dimensions of encoder and decoder must be equal!\"\n",
    "        \n",
    "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
    "        \n",
    "        #src = [src len, batch size]\n",
    "        #trg = [trg len, batch size]\n",
    "        #teacher_forcing_ratio is probability to use teacher forcing\n",
    "        #e.g. if teacher_forcing_ratio is 0.75 we use ground-truth inputs 75% of the time\n",
    "        \n",
    "        batch_size = trg.shape[1]\n",
    "        trg_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        \n",
    "        #tensor to store decoder outputs\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        \n",
    "        #last hidden state of the encoder is the context\n",
    "        context = self.encoder(src)\n",
    "        \n",
    "        #context also used as the initial hidden state of the decoder\n",
    "        hidden = context\n",
    "        \n",
    "        #first input to the decoder is the <sos> tokens\n",
    "        input = trg[0,:]\n",
    "        \n",
    "        for t in range(1, trg_len):\n",
    "            \n",
    "            #insert input token embedding, previous hidden state and the context state\n",
    "            #receive output tensor (predictions) and new hidden state\n",
    "            output, hidden = self.decoder(input, hidden, context)\n",
    "            \n",
    "            #place predictions in a tensor holding predictions for each token\n",
    "            outputs[t] = output\n",
    "            \n",
    "            #decide if we are going to use teacher forcing or not\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            \n",
    "            #get the highest predicted token from our predictions\n",
    "            top1 = output.argmax(1) \n",
    "            \n",
    "            #if teacher forcing, use actual next token as next input\n",
    "            #if not, use predicted token\n",
    "            input = trg[t] if teacher_force else top1\n",
    "\n",
    "        return outputs"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rnt59xRsPjQJ"
   },
   "source": [
    "# Обучение модели Seq2Seq\n",
    "\n",
    "Остальная часть этой статьи очень похожа на аналогичную часть из предыдущей статьи.\n",
    "\n",
    "Мы инициализируем наш кодер, декодер и модель seq2seq (поместив его на графический процессор, если он у нас есть). Как и раньше, размеры эмбеддинга и величина дропаута могут быть разными для кодера и декодера, но скрытые размеры должны оставаться такими же."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "bfWJAHYzPjQK"
   },
   "source": [
    "INPUT_DIM = len(SRC.vocab)\n",
    "OUTPUT_DIM = len(TRG.vocab)\n",
    "ENC_EMB_DIM = 256\n",
    "DEC_EMB_DIM = 256\n",
    "HID_DIM = 512\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, ENC_DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, DEC_DROPOUT)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = Seq2Seq(enc, dec, device).to(device)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "114PBD3zPjQK"
   },
   "source": [
    "Затем мы инициализируем наши параметры. В исходной статье говорится, что параметры инициализируются из нормального распределения со средним значением 0 и стандартным отклонением0 .01, т.е. $\\mathcal{N}(0, 0.01)$.\n",
    "\n",
    "В ней также говорится, что мы должны инициализировать повторяющиеся параметры специальным образом, однако для простоты мы инициализируем их в виде $\\mathcal{N}(0, 0.01)$."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "EMCVpvE_PjQL",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "ab6cd702-ef89-4d4b-9174-bae2ed271338"
   },
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "        \n",
    "model.apply(init_weights)"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(7853, 256)\n",
       "    (rnn): GRU(256, 512)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(5893, 256)\n",
       "    (rnn): GRU(768, 512)\n",
       "    (fc_out): Linear(in_features=1280, out_features=5893, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {
      "tags": []
     },
     "execution_count": 15
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bnomWUPWPjQM"
   },
   "source": [
    "Распечатываем количество параметров.\n",
    "\n",
    "Несмотря на то, что у нас есть только однослойная RNN для нашего кодера и декодера, на самом деле у нас есть **больше** параметры, чем в предыдущей модели. Это связано с увеличенным размером входов в GRU и линейный слой. Однако это незначительное увеличение параметров не приводит увеличению времени обучения (~3 секунд на дополнительную эпоху)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "jyFwfhAyPjQN",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "104df0b3-a441-4886-bf3b-005b7249526e"
   },
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "The model has 14,219,781 trainable parameters\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T682CV9RPjQN"
   },
   "source": [
    "Мы инициализируем наш оптимизатор."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "VD2PC9x_PjQN"
   },
   "source": [
    "optimizer = optim.Adam(model.parameters())"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AX-8GWtRPjQO"
   },
   "source": [
    "Мы также инициализируем функцию потерь, игнорируя потерю на токенах `<pad>`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "CA272wwpPjQO"
   },
   "source": [
    "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ijOjmCcwPjQP"
   },
   "source": [
    "Затем мы создаем цикл обучения ..."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "vXNPb5lyPjQP"
   },
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, batch in enumerate(iterator):\n",
    "        \n",
    "        src = batch.src\n",
    "        trg = batch.trg\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(src, trg)\n",
    "        \n",
    "        #trg = [trg len, batch size]\n",
    "        #output = [trg len, batch size, output dim]\n",
    "        \n",
    "        output_dim = output.shape[-1]\n",
    "        \n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        trg = trg[1:].view(-1)\n",
    "        \n",
    "        #trg = [(trg len - 1) * batch size]\n",
    "        #output = [(trg len - 1) * batch size, output dim]\n",
    "        \n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VbHLHEFSPjQP"
   },
   "source": [
    "...и цикл оценки, не забывая установить модель в режим `eval` и выключить обучение с принуждением."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "YoKMAdqkPjQQ"
   },
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i, batch in enumerate(iterator):\n",
    "\n",
    "            src = batch.src\n",
    "            trg = batch.trg\n",
    "\n",
    "            output = model(src, trg, 0) #turn off teacher forcing\n",
    "\n",
    "            #trg = [trg len, batch size]\n",
    "            #output = [trg len, batch size, output dim]\n",
    "\n",
    "            output_dim = output.shape[-1]\n",
    "            \n",
    "            output = output[1:].view(-1, output_dim)\n",
    "            trg = trg[1:].view(-1)\n",
    "\n",
    "            #trg = [(trg len - 1) * batch size]\n",
    "            #output = [(trg len - 1) * batch size, output dim]\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_wdW-p_oPjQQ"
   },
   "source": [
    "Мы также определим функцию, которая вычисляет, сколько времени занимает эпоха."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "SZ6D1cL6PjQQ"
   },
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zl2wa7tBPjQR"
   },
   "source": [
    "Затем мы обучаем нашу модель, сохраняя параметры, которые дают нам наименьшие потери при проверке."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "VMfppNHEPjQR",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "9118c828-baf4-48ef-8ce0-e6444d4b68f6"
   },
   "source": [
    "N_EPOCHS = 10\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut2-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 0m 28s\n",
      "\tTrain Loss: 5.072 | Train PPL: 159.506\n",
      "\t Val. Loss: 5.383 |  Val. PPL: 217.765\n",
      "Epoch: 02 | Time: 0m 27s\n",
      "\tTrain Loss: 4.382 | Train PPL:  80.016\n",
      "\t Val. Loss: 5.128 |  Val. PPL: 168.758\n",
      "Epoch: 03 | Time: 0m 27s\n",
      "\tTrain Loss: 4.017 | Train PPL:  55.524\n",
      "\t Val. Loss: 4.666 |  Val. PPL: 106.295\n",
      "Epoch: 04 | Time: 0m 27s\n",
      "\tTrain Loss: 3.630 | Train PPL:  37.701\n",
      "\t Val. Loss: 4.263 |  Val. PPL:  71.011\n",
      "Epoch: 05 | Time: 0m 27s\n",
      "\tTrain Loss: 3.282 | Train PPL:  26.618\n",
      "\t Val. Loss: 4.025 |  Val. PPL:  55.963\n",
      "Epoch: 06 | Time: 0m 27s\n",
      "\tTrain Loss: 2.988 | Train PPL:  19.845\n",
      "\t Val. Loss: 3.878 |  Val. PPL:  48.329\n",
      "Epoch: 07 | Time: 0m 27s\n",
      "\tTrain Loss: 2.743 | Train PPL:  15.538\n",
      "\t Val. Loss: 3.691 |  Val. PPL:  40.102\n",
      "Epoch: 08 | Time: 0m 27s\n",
      "\tTrain Loss: 2.502 | Train PPL:  12.209\n",
      "\t Val. Loss: 3.604 |  Val. PPL:  36.761\n",
      "Epoch: 09 | Time: 0m 27s\n",
      "\tTrain Loss: 2.289 | Train PPL:   9.864\n",
      "\t Val. Loss: 3.586 |  Val. PPL:  36.094\n",
      "Epoch: 10 | Time: 0m 27s\n",
      "\tTrain Loss: 2.131 | Train PPL:   8.421\n",
      "\t Val. Loss: 3.618 |  Val. PPL:  37.266\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lyfea8qgPjQS"
   },
   "source": [
    "Наконец, мы тестируем модель на тестовой выборке, используя эти «лучшие» параметры."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "_yM-yUwDPjQS",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "662265f9-21f0-4764-c6d4-225ea6f72958"
   },
   "source": [
    "model.load_state_dict(torch.load('tut2-model.pt'))\n",
    "\n",
    "test_loss = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "| Test Loss: 3.589 | Test PPL:  36.207 |\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vic56cIOPjQT"
   },
   "source": [
    "Если посмотреть на выигрыш в тесте, то видно улучшение производительность по сравнению с предыдущей моделью. Это довольно хороший признак того, что эта архитектура модели что-то делает лучше! Ослабление сжатия информации кажется неплохим подходом, и в следующей статье мы пойдём по этому пути еще дальше с помощью *внимания*."
   ]
  }
 ]
}